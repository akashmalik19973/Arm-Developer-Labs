---
layout: page
title: Challenge Page Registration
show_title: false
---

<div class="challenge-wrapper">
  <header class="challenge-header">
    <div class="ch-header-content">
      <div class="ch-badge">
        <span class="ch-badge-dot"></span>
        <span>Bharat AI-SoC Student Challenge</span>
      </div>

      <h1>Bharat AI-SoC Student Challenge</h1>

      <p class="ch-tagline">
        A project-based virtual challenge to ignite innovation in AI-driven System-on-Chip (SoC) design using Arm architecture.
      </p>

      <div class="ch-header-meta">
        <div class="ch-meta-pill">
          <span class="ch-meta-label">Mode</span>
          <span class="ch-meta-value">Virtual Project Challenge</span>
        </div>
        <div class="ch-meta-pill">
          <span class="ch-meta-label">Team Size</span>
          <span class="ch-meta-value">1–3 Students</span>
        </div>
        <div class="ch-meta-pill">
          <span class="ch-meta-label">Eligibility</span>
          <span class="ch-meta-value">Indian Institutes Only</span>
        </div>
      </div>
    </div>
  </header>

  <main class="ch-main">
    <!-- LEFT COLUMN -->
    <div>
      <section class="ch-section">
        <h2>Objective</h2>
        <p>
          Enhance industry-relevant skills through project-based learning in the space of AI and SoC via an experiential mini-project.
        </p>
        <p>
          To ignite a culture of innovation by empowering students to ideate next-generation SoC solutions that unite AI
          and sustainability, leveraging Arm architecture—preparing them to shape the semiconductor future.
        </p>
      </section>

      <section class="ch-section">
        <h2>Eligibility Criteria</h2>
        <ol>
          <li>Participants must be Indian nationals.</li>
          <li>Participants must be associated with an Indian institute.</li>
          <li>Students must be nominated by their respective college.</li>
          <li>Teams of 1–3 students can apply.</li>
          <li>All team members must be from the same college.</li>
          <li>Team must have a Team Leader & Faculty Mentor.</li>
          <li>Participants must understand Digital System Design & FPGA flow.</li>
        </ol>
      </section>

      <section class="ch-section">
        <h2>Challenge Guidelines & Terms</h2>
        <ul>
          <li>Only eligible students can participate.</li>
          <li>Team size: 1–3 members.</li>
          <li>Only Team Leader registers for the team.</li>
          <li>Duplicate registration results in rejection.</li>
          <li>Registration details must be accurate.</li>
          <li>Mentoring by Arm experts (virtual) & college mentors (local).</li>
          <li>Teams can reach Arm experts via email/Slack/Discord.</li>
          <li>Mini project to be submitted on Arm-Developer-Labs via GitHub.</li>
          <li>Arm may assign a different problem statement.</li>
          <li>Winners must provide college ID & documents.</li>
          <li>Updates will be posted on this page.</li>
          <li>Support: <strong>Armwinterchallenge@arm.com</strong></li>
        </ul>

        <div class="ch-note">
          <strong>Note:</strong> Incorrect or duplicate entries may result in disqualification.
        </div>
      </section>
    </div>

    <!-- RIGHT COLUMN -->
    <div>
      <section class="ch-section">
        <h2>Detailed Timeline</h2>

        <table class="ch-table">
          <thead>
            <tr>
              <th>Activity</th>
              <th>Description</th>
              <th>Start</th>
              <th>End</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Registration</td>
              <td>Register using the link on this page</td>
              <td>10 Dec 2025</td>
              <td>24 Dec 2025</td>
            </tr>
            <tr>
              <td>Mentoring Session</td>
              <td>Online sessions by Arm experts</td>
              <td>16 Dec 2025</td>
              <td>24 Dec 2025</td>
            </tr>
            <tr>
              <td>Challenge Launch</td>
              <td>Official challenge start</td>
              <td>25 Dec 2025</td>
              <td>31 Jan 2026</td>
            </tr>
            <tr>
              <td>Project Submission – Form</td>
              <td>Submission form opens</td>
              <td>25 Jan 2026</td>
              <td>26 Jan 2026</td>
            </tr>
            <tr>
              <td>Project Submission – Upload</td>
              <td>Upload project online</td>
              <td>25 Jan 2026</td>
              <td>31 Jan 2026</td>
            </tr>
            <tr>
              <td>Project Evaluation</td>
              <td>Arm experts shortlist top 25 teams</td>
              <td>1 Feb 2026</td>
              <td>28 Feb 2026</td>
            </tr>
            <tr>
              <td>Finals</td>
              <td>Virtual pitch by shortlisted teams</td>
              <td>2 Mar 2026</td>
              <td>4 Mar 2026</td>
            </tr>
            <tr>
              <td>Winners Announcement</td>
              <td>Winners published on webpage</td>
              <td>4 Mar 2026</td>
              <td>5 Mar 2026</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section class="ch-section">
        <h2>Support & Queries</h2>
        <p>For support, email:</p>
        <p><a href="mailto:Armwinterchallenge@arm.com" class="ch-email">Armwinterchallenge@arm.com</a></p>
        <p class="ch-small">All updates will be posted on this page.</p>
      </section>
    </div>
  </main>
</div>

<!--Project Section-->

<section class="ch-section ch-projects">
  <h2>Project Problem Statements</h2>
  <p class="ch-projects-intro">
    Choose one of the following problem statements. Click to expand each project and view full details.
  </p>

  <!-- Problem 1 -->
  <details class="ch-accordion" open>
    <summary>
      <span class="ch-acc-title">Problem Statement 1</span>
      <span class="ch-acc-subtitle">
        Real-Time Object Detection Using Hardware-Accelerated CNN on Xilinx Zynq FPGA with Arm Processor
      </span>
    </summary>
    <div class="ch-acc-body">
      <h3>Background</h3>
      <p>
        Embedded systems are increasingly used for edge AI tasks such as object detection, classification and tracking.
        Pure CPU-based implementations often fail to meet real-time constraints on resource-constrained platforms.
        Xilinx Zynq SoCs integrate an Arm core with FPGA fabric, making them ideal for hardware-accelerated CNN inference.
      </p>

      <h3>Objectives</h3>
      <ul>
        <li>Implement a lightweight CNN (e.g., Tiny-YOLO, MobileNet or a custom 3-layer CNN) for object detection or image classification.</li>
        <li>Partition the design between:
          <ul>
            <li><strong>Arm core</strong>: image capture, preprocessing, control logic.</li>
            <li><strong>FPGA fabric</strong>: CNN compute (convolution, pooling) via Vitis HLS / Vivado.</li>
          </ul>
        </li>
        <li>Demonstrate real-time inference using a test dataset or live camera feed.</li>
        <li>Compare latency, throughput and power for hardware-accelerated vs software-only versions.</li>
      </ul>

      <h3>System Architecture</h3>
      <ul>
        <li><strong>Input</strong>: Camera or dataset (e.g., CIFAR-10, live USB camera).</li>
        <li><strong>Processing</strong>:
          <ul>
            <li>Arm: capture + preprocess image data.</li>
            <li>FPGA: convolution, ReLU, pooling.</li>
            <li>Arm: post-process and display result.</li>
          </ul>
        </li>
        <li><strong>Output</strong>: Class label or bounding box via display / serial console.</li>
      </ul>

      <h3>Tools &amp; Technologies</h3>
      <ul>
        <li><strong>Hardware</strong>: Xilinx Zynq-7000 / ZCU104 / ZedBoard.</li>
        <li><strong>Software</strong>:
          <ul>
            <li>Xilinx Vivado (FPGA design), Vitis / SDSoC (HW/SW co-design).</li>
            <li>PetaLinux (optional), OpenCV on Arm.</li>
            <li>Python / C++ for control software.</li>
            <li>Verilog / HLS C++ for accelerator design.</li>
          </ul>
        </li>
      </ul>

      <h3>Suggested Timeline (2–3 Weeks)</h3>
      <table class="ch-table ch-table-compact">
        <thead>
          <tr>
            <th>Week</th>
            <th>Tasks</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Week 1</td>
            <td>Literature review, board setup, test image I/O pipeline.</td>
          </tr>
          <tr>
            <td>Week 2</td>
            <td>CPU CNN implementation, FPGA accelerator via HLS, integration with Arm.</td>
          </tr>
          <tr>
            <td>Week 3</td>
            <td>Inference tests, latency/throughput measurement, optimizations, documentation and demo.</td>
          </tr>
        </tbody>
      </table>

      <h3>Expected Outcomes</h3>
      <ul>
        <li>Working prototype performing object detection / classification on FPGA.</li>
        <li>At least 2× speedup vs software-only implementation.</li>
        <li>Performance analysis (latency, throughput, resource usage, power).</li>
        <li>Documented HW/SW co-design process.</li>
      </ul>

      <h3>Possible Simplifications</h3>
      <ul>
        <li>Use smaller CNN or grayscale images.</li>
        <li>Accelerate only convolution in hardware.</li>
        <li>Use pre-trained weights (e.g., MNIST).</li>
      </ul>

      <h3>Extensions</h3>
      <ul>
        <li>Add DMA between Arm and FPGA.</li>
        <li>Multiple parallel convolution engines.</li>
        <li>Target a specific application (face detection, traffic sign recognition, etc.).</li>
      </ul>
    </div>
  </details>

  <!-- Problem 2 -->
  <details class="ch-accordion">
    <summary>
      <span class="ch-acc-title">Problem Statement 2</span>
      <span class="ch-acc-subtitle">
        Offline, Privacy-Preserving Hindi Voice Assistant on Raspberry Pi
      </span>
    </summary>
    <div class="ch-acc-body">
      <h3>Objective</h3>
      <p>
        Develop a low-latency, privacy-preserving voice assistant on an Arm-based SBC (e.g., Raspberry Pi) that
        processes Hindi voice commands entirely offline. The assistant should handle local queries (time, weather, etc.)
        using on-device ASR and TTS.
      </p>

      <h3>Project Description</h3>
      <p>
        Students will build an embedded speech pipeline performing:
      </p>
      <ul>
        <li>Speech-to-text using a lightweight ASR model (e.g., Coqui STT or fine-tuned wav2vec2 for Hindi).</li>
        <li>Command parsing and intent recognition in Python.</li>
        <li>Text-to-speech responses using local TTS (eSpeak-NG or Festival).</li>
        <li>End-to-end on the Raspberry Pi CPU with no cloud dependency.</li>
      </ul>

      <h3>Key Requirements</h3>
      <ul>
        <li><strong>Hardware</strong>:
          <ul>
            <li>Raspberry Pi 4 (or similar Arm SBC).</li>
            <li>USB microphone.</li>
            <li>Speaker via 3.5 mm jack or HDMI.</li>
          </ul>
        </li>
        <li><strong>Software</strong>:
          <ul>
            <li>Python with PyAudio for audio I/O.</li>
            <li>Coqui STT or fine-tuned wav2vec2 for ASR.</li>
            <li>eSpeak-NG or Festival for TTS.</li>
            <li>Custom Python logic for intent recognition and command execution.</li>
          </ul>
        </li>
      </ul>

      <h3>Performance Targets</h3>
      <ul>
        <li>Sub-2-second response time per command.</li>
        <li>Accurate recognition for 10–15 Hindi commands.</li>
        <li>Robust, fully offline operation.</li>
      </ul>

      <h3>Deliverables</h3>
      <ul>
        <li>Source code for the full voice assistant pipeline.</li>
        <li>Documentation of any model fine-tuning / optimization steps.</li>
        <li>Demo video showing responses to multiple commands.</li>
        <li>Short report on architecture, challenges in Hindi ASR/TTS, and performance metrics.</li>
      </ul>

      <h3>Learning Outcomes</h3>
      <ul>
        <li>Hands-on experience with embedded speech AI and offline ASR/TTS.</li>
        <li>Understanding challenges of regional language processing.</li>
        <li>Integrating ASR, simple NLP/intent logic, and TTS on a constrained platform.</li>
      </ul>
    </div>
  </details>

  <!-- Problem 3 -->
  <details class="ch-accordion">
    <summary>
      <span class="ch-acc-title">Problem Statement 3</span>
      <span class="ch-acc-subtitle">
        Touchless HCI for Media Control Using Hand Gestures on NVIDIA Jetson Nano
      </span>
    </summary>
    <div class="ch-acc-body">
      <h3>Objective</h3>
      <p>
        Create a touchless HCI system using an NVIDIA Jetson Nano that translates real-time hand gestures into media
        control commands (e.g., play/pause, volume) for a local player such as VLC.
      </p>

      <h3>Project Description</h3>
      <p>
        Students will use MediaPipe Hands (optimized for Arm CPU and Jetson GPU) to detect hand landmarks, then classify
        gestures and map them to keyboard shortcuts using Python libraries like <code>pynput</code> or <code>xdotool</code>.
      </p>

      <h3>Key Requirements</h3>
      <ul>
        <li><strong>Hardware</strong>:
          <ul>
            <li>NVIDIA Jetson Nano Developer Kit.</li>
            <li>USB webcam.</li>
            <li>Monitor and standard peripherals.</li>
          </ul>
        </li>
        <li><strong>Software</strong>:
          <ul>
            <li>JetPack OS with CUDA support.</li>
            <li>Python, OpenCV, MediaPipe.</li>
            <li>Media player application (e.g., VLC).</li>
          </ul>
        </li>
      </ul>

      <h3>Performance Targets</h3>
      <ul>
        <li>&gt;90% gesture recognition accuracy in controlled lighting.</li>
        <li>&lt;200 ms end-to-end latency for gesture → action.</li>
        <li>Stable at ≥15 FPS.</li>
      </ul>

      <h3>Deliverables</h3>
      <ul>
        <li>Source code for gesture recognition and control logic.</li>
        <li>Defined gesture set and mapping table.</li>
        <li>Demo video of real-time media control.</li>
        <li>Report on design, model choice and performance analysis.</li>
      </ul>

      <h3>Learning Outcomes</h3>
      <ul>
        <li>Practical experience with real-time edge computer vision.</li>
        <li>Pipeline optimization for low-latency inference.</li>
        <li>Integrating AI perception with system-level control.</li>
      </ul>
    </div>
  </details>

  <!-- Problem 4 -->
  <details class="ch-accordion">
    <summary>
      <span class="ch-acc-title">Problem Statement 4</span>
      <span class="ch-acc-subtitle">
        Real-Time Road Anomaly Detection from Dashcam Footage on Raspberry Pi
      </span>
    </summary>
    <div class="ch-acc-body">
      <h3>Objective</h3>
      <p>
        Build an edge AI application on Raspberry Pi that processes dashcam footage in real-time to detect and log road
        anomalies such as potholes and unexpected obstacles.
      </p>

      <h3>Project Description</h3>
      <p>
        Students will choose a lightweight object detector (e.g., MobileNet-SSD, YOLOv5s), convert it to an
        edge-optimized format (TensorFlow Lite / ONNX Runtime), and integrate it with an OpenCV video pipeline.
        Detected anomalies should trigger timestamped logs or saved clips.
      </p>

      <h3>Key Requirements</h3>
      <ul>
        <li><strong>Hardware</strong>:
          <ul>
            <li>Raspberry Pi 4 with proper cooling.</li>
            <li>Raspberry Pi Camera Module v2 or USB webcam.</li>
            <li>High-write-speed microSD card.</li>
          </ul>
        </li>
        <li><strong>Software</strong>:
          <ul>
            <li>Raspberry Pi OS.</li>
            <li>Python, OpenCV.</li>
            <li>TensorFlow Lite / ONNX Runtime with a pre-trained, quantized detection model.</li>
          </ul>
        </li>
      </ul>

      <h3>Performance Targets</h3>
      <ul>
        <li>≥5 FPS near-real-time inference.</li>
        <li>High precision to reduce false positives in logging.</li>
        <li>Robust under varying lighting conditions.</li>
      </ul>

      <h3>Deliverables</h3>
      <ul>
        <li>Source code for video processing and inference pipeline.</li>
        <li>Optimized deployed model file (.tflite / .onnx).</li>
        <li>Demo video with anomaly detection on sample footage.</li>
        <li>Report on model choice, optimization and performance.</li>
      </ul>

      <h3>Learning Outcomes</h3>
      <ul>
        <li>Optimizing and deploying neural networks for edge video analytics.</li>
        <li>Experience with embedded vision pipelines.</li>
        <li>Understanding accuracy vs speed vs compute trade-offs on Arm platforms.</li>
      </ul>
    </div>
  </details>

  <!-- Problem 5 -->
  <details class="ch-accordion">
    <summary>
      <span class="ch-acc-title">Problem Statement 5</span>
      <span class="ch-acc-subtitle">
        Hardware-Accelerated Autonomous Robot with ROS 2 on Xilinx Kria SoC
      </span>
    </summary>
    <div class="ch-acc-body">
      <h3>Objective</h3>
      <p>
        Develop an autonomous robot using ROS 2 on a heterogeneous SoC platform (e.g., Xilinx Kria KR260) where
        compute-intensive perception nodes are accelerated in the Programmable Logic (PL) to achieve real-time performance.
      </p>

      <h3>Project Description</h3>
      <p>
        Students will build a robot chassis around the Kria SoM, identify bottlenecks in a ROS 2 perception pipeline
        (e.g., image rectification, feature detection, point cloud processing), and implement these as hardware
        accelerators using Vitis HLS. The custom IP cores will be integrated into the PL and exposed to ROS 2 nodes
        running on the Arm cores.
      </p>

      <h3>Key Requirements</h3>
      <ul>
        <li><strong>Hardware</strong>:
          <ul>
            <li>Xilinx Kria KR260 Robotics Starter Kit / custom robot.</li>
            <li>Robot chassis, motors and motor drivers.</li>
            <li>2D LiDAR (e.g., RPLIDAR A1) or stereo camera.</li>
            <li>IMU sensor.</li>
          </ul>
        </li>
        <li><strong>Software &amp; Tools</strong>:
          <ul>
            <li>Vitis Unified Software Platform and Vitis HLS.</li>
            <li>Kria Robotics Stack (KRS) or ROS 2 Humble on Ubuntu.</li>
            <li>Python / C++ for ROS 2 node development.</li>
          </ul>
        </li>
      </ul>

      <h3>Performance Targets</h3>
      <ul>
        <li>&gt;5× FPS improvement in an accelerated node vs software-only.</li>
        <li>Stable ROS 2 navigation stack (localization, mapping, path planning).</li>
        <li>Successful autonomous task (e.g., corridor navigation, obstacle avoidance).</li>
      </ul>

      <h3>Deliverables</h3>
      <ul>
        <li>Source code for all ROS 2 nodes (software and HW-accelerated interfaces).</li>
        <li>Vitis HLS code and hardware design files (XSA) for accelerator IP.</li>
        <li>Demo video of autonomous navigation with and without acceleration (comparison).</li>
        <li>4–6 page report on architecture, accelerator design, integration and performance analysis.</li>
      </ul>

      <h3>Learning Outcomes</h3>
      <ul>
        <li>Integration of ROS 2 with heterogeneous SoC acceleration.</li>
        <li>Skills in HW/SW co-design and performance profiling.</li>
        <li>Experience with full FPGA accelerator flow from HLS to system integration.</li>
        <li>Understanding modern robotics SoC workflows (Kria platform).</li>
      </ul>
    </div>
  </details>
</section>
<a href="" class="ch-floating-register">Register</a>